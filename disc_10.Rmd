---
title: "Discussion 10"
author: "Cong Xu"
date: "3/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## hw5 q1

1. Coefficient of determination $R^2$. In simple linear regression ($y_i= \beta_0 + \beta_1 x_i + \epsilon_i$)

```{r}
x <- rnorm(1000)
y <- x + rnorm(1000)
summary(lm(y~x))$r.squared
cor(x,y)^2
```

Why are they the same?

$$
\begin{eqnarray*}
  \hat{\beta}_1 &=& \frac{S_{xy}}{S_{xx}}\\
  \hat{\beta}_0 &=& \bar{y} - \hat{\beta_1} \bar{x}\\
  R^2 &=& 1 - \frac{SSE}{SSTO}
\end{eqnarray*}
$$

$$
\begin{eqnarray*}
  SSE &=& \sum_{i=1}^n (y_i - \hat{y}_i)^2\\
  &=& \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta_1}x_i)^2\\
  &=& \sum_{i=1}^n (y_i - (\bar{y} - \hat{\beta_1} \bar{x}) - \hat{\beta_1}x_i)\\
  &=& \sum_{i=1}^n (y_i - \bar{y} - \hat{\beta_1}(x_i - \bar{x}))^2\\
  &=& \sum_{i=1}^n (y_i - \bar{y})^2 - 2 \hat{\beta_1} \sum_{i=1}^n (y_i - \bar{y}) (x_i - \bar{x}) + \hat{\beta_1}^2 \sum_{i=1}^n (x_i - \bar{x})^2\\
  &=& S_{yy} - 2 \hat{\beta_1} S_{xy} + \hat{\beta_1}^2 S_{xx}\\
  &=& S_{yy} - 2 (\frac{S_{xy}}{S_{xx}}) S_{xy} + (\frac{S_{xy}}{S_{xx}})^2 S_{xx}\\
  &=& S_{yy} - \frac{S_{xy}^2}{S_{xx}}
\end{eqnarray*}
$$
$$
  SSTO = \sum_{i=1}^n (y_i - \bar{y})^2 = S_{yy}
$$

$$
\begin{eqnarray*}
  R^2 &=& 1 - \frac{SSE}{SSTO}\\
  &=& 1 - \frac{S_{yy} - \frac{S_{xy}^2}{S_{xx}}}{S_{yy}}\\
  &=& \frac{S_{xy}^2}{S_{xx}S_{yy}}\\
  &=& (\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}})^2\\
  &=& (\hat{Cor}(\vec{x}, \vec{y}))^2
\end{eqnarray*}
$$

2. Argument `formula` and argument `data` for `lm`.

```{r}
df <- read.csv('data.csv')
summary(lm(y ~ x))
summary(lm(y ~ x, data = df))
```

3. Weighted Least Squared 

For linear regression, ordinary least squared

$$
  \hat{\beta} = arg\min_{\beta} \sum_{i=1}^n (y_i - \vec{x}_i^{T} \beta)^2
$$

And for weighted least square

$$
  \hat{\beta} = arg\min_{\beta} \sum_{i=1}^n w_i(y_i - \vec{x}_i^{T} \beta)^2
$$

Specifically, for one BLB sample

```{r}
get_data <- function(i) {
  df = read.csv('data.csv', skip = (i-1)*100+1, nrows=100, header=FALSE)
  colnames(df) = c('x', 'y')
  return(df)
}
subsample <- get_data(1)
dim(subsample)
```

```{r}
n <- dim(df)[1]
b <- dim(subsample)[1]
w <- c(rmultinom(1, n, rep(1,b))) # weights
w
```

`w` denotes the numbers of replication for each observation. Therefore, the ordinary least square of the BLB sample is equivalent to the weighted least square of the initial subsample, with `w` as the weights.

```{r}
summary(lm(y ~ x, weights=w, data=subsample))
```

### q2

```{r}
library(reticulate)
```

Tell `reticulate` to use `miniconda`.
```{r}
use_miniconda(required = TRUE)
```

```{python}
def kendall_a(x, y):
  n = x.shape[0]
  nc = nd = 0
  for i in range(n-1):
    for j in range(i+1, n):
      if (x[i]-x[j])*(y[i]-y[j]) > 0:
        nc += 1
      else: # (x[i]-x[j])*(y[i]-y[j]) < 0 as there is no tie
        nd += 1
  tau = float(nc - nd)/( (n*(n - 1))/2 )
  return tau

```

```{r}
n <- 1000
# R's array will be casted as Python's numpy array
x <- as.array(runif(n))
y <- as.array(runif(n))
py$kendall_a(x, y)
cor(x, y, method = "kendall")
```

https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient

```{python}
def kendall_b(x, y):
  n = x.shape[0]
  nc = nd = 0
  for i in range(n-1):
    for j in range(i+1, n):
      if (x[i]-x[j])*(y[i]-y[j]) > 0:
        nc += 1
      elif (x[i]-x[j])*(y[i]-y[j]) < 0:
        nd += 1
  t = np.unique(x, return_counts=True)[1]
  u = np.unique(y, return_counts=True)[1]
  n1 = np.sum(t*(t-1))/2
  n2 = np.sum(u*(u-1))/2
  n0 = n*(n-1)/2
  tau = (nc-nd)/np.sqrt((n0-n1)*(n0-n2))
  return tau

```

```{python}
import numpy as np
from math import floor
n = 1000
x = np.random.choice(floor(n * 0.9), n)
y = np.random.choice(floor(n * 0.9), n)
np.unique(x, return_counts=True)
```

```{r}
n <- 1000
# R's array will be casted as Python's numpy array
x <- as.array(sample.int(floor(n * 0.9), n, replace = TRUE))
y <- as.array(sample.int(floor(n * 0.9), n, replace = TRUE))
py$kendall_b(x, y)
cor(x, y, method = "kendall")
```